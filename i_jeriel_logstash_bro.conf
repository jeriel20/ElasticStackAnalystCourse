########################
# logstash Config File
# CPT 152
# 20160703
# SSG Juarbe
#######################

######################
#					
#	BRO-CONN INPUT	
#
######################
input {
  file {
    type => "bro-conn_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_conn_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-conn.*"
    tags => [ "tlaw" ]
    tags => [ "bro_conn" ]
  }
#######################
#					
#	BRO-HTTP INPUT	
#
######################
	file {
		type => "bro-http_log"
		start_position => "beginning"
		sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_http_sincedb"

		#Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
		path => "/data/usareur-tlaw-nj/*bro-http*"
		tags => [ "tlaw" ]
		tags => [ "bro_http" ]
	}
#######################
#					
#	BRO-DNS INPUT	
#
######################
	file {
		type => "bro-dns_log"
		start_position => "beginning"
		sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_dns_sincedb"

		#Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
		path => "/data/usareur-tlaw-nj/*bro-dns*"
		tags => [ "tlaw" ]
		tags => [ "bro_dns" ]
	}
#######################
#					
#	BRO-FILES INPUT	
#
######################
	file {
		type => "bro-files_log"
		start_position => "beginning"
		sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_files_sincedb"

		#Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
		path => "/data/usareur-tlaw-nj/*bro-files*"
		tags => [ "tlaw" ]
		tags => [ "bro_files" ]
	}
#######################
#					
#	BRO-FTP INPUT	
#
######################
	file {
		type => "bro-ftp_log"
		start_position => "beginning"
		sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_ftp_sincedb"

		#Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
		path => "/data/usareur-tlaw-nj/*bro-ftp*"
		tags => [ "tlaw" ]
		tags => [ "bro_ftp" ]
	}
#######################
#					
#	BRO-NOTICE INPUT	
#
######################
	file {
		type => "bro-notice_log"
		start_position => "beginning"
		sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_notice_sincedb"

		#Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
		path => "/data/usareur-tlaw-nj/*bro-notice*"
		tags => [ "tlaw" ]
		tags => [ "bro_notice" ]
	}
#######################
#					
#	BRO-WEIRD INPUT	
#
######################
	file {
		type => "bro-weird_log"
		start_position => "beginning"
		sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_weird_sincedb"

		#Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
		path => "/data/usareur-tlaw-nj/*bro-weird*"
		tags => [ "tlaw" ]
		tags => [ "bro_weird" ]
	}
#######################
#					
#	BRO-SNMP INPUT	
#
######################
  file {
    type => "bro-snmp_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_snmp_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-snmp*"
    tags => [ "tlaw" ]
    tags => [ "bro_snmp" ]
	}
#######################
#					
#	BRO-RADIUS INPUT	
#
######################
  file {
    type => "bro-radius_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_radius_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-radius*"
    tags => [ "tlaw" ]
    tags => [ "bro_radius" ]
	}
#######################
#					
#	BRO-SOCKS INPUT	
#
######################
  file {
    type => "bro-socks_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_socks_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-socks*"
    tags => [ "tlaw" ]
    tags => [ "bro_socks" ]
	}
#######################
#					
#	BRO-SMTP INPUT	
#
######################	
  file {
    type => "bro-smtp_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_smtp_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-smtp*"
    tags => [ "tlaw" ]
    tags => [ "bro_smtp" ]
	}
#######################
#					
#	BRO-SOFTWARE INPUT	
#
######################
  file {
    type => "bro-software_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_software_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-software*"
    tags => [ "tlaw" ]
    tags => [ "bro_software" ]
	}
#######################
#					
#	BRO-SSH INPUT	
#
######################
  file {
    type => "bro-ssh_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_ssh_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-ssh*"
    tags => [ "tlaw" ]
    tags => [ "bro_ssh" ]
    }
#######################
#					
#	BRO-REPORTER INPUT	
#
######################
  file {
    type => "bro-reporter_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_reporter_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-reporter*"
    tags => [ "tlaw" ]
    tags => [ "bro_reporter" ]
  }
#######################
#					
#	BRO-SSL INPUT	
#
######################
  file {
    type => "bro-ssl_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_ssl_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-ssl*"
    tags => [ "tlaw" ]
    tags => [ "bro_ssl" ]
  }
#######################
#					
#	BRO-X509 INPUT	
#
######################
  file {
    type => "bro-x509_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_x509_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-x509*"
    tags => [ "tlaw" ]
    tags => [ "bro_x509" ]
  }
#######################
#					
#	BRO-TUNNEL INPUT	
#
######################
  file {
    type => "bro-tunnel_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_tunnel_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-tunnel*"
    tags => [ "tlaw" ]
    tags => [ "bro_tunnel" ]
  }
#######################
#					
#	BRO-COMMUNICATION INPUT	
#
######################
  file {
    type => "bro-communication_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_communication_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-communication*"
    tags => [ "tlaw" ]
    tags => [ "bro_communication" ]
  }
#######################
#					
#	BRO-DPD INPUT	
#
######################
  file {
    type => "bro-dpd_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_dpd_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-dpd*"
    tags => [ "tlaw" ]
    tags => [ "bro_dpd" ]
  }
#######################
#					
#	BRO-KNOWN_HOSTS INPUT	
#
######################
  file {
    type => "bro-known_hosts_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_known_hosts_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-known_hosts*"
    tags => [ "tlaw" ]
    tags => [ "bro_known_hosts" ]
  }
#######################
#					
#	BRO-KNOWN_SERVICES INPUT	
#
######################
  file {
    type => "bro-known_services_log"
    start_position => "beginning"
    sincedb_path => "/data/usareur-tlaw-nj/since_db/.bro_known_services_sincedb"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "/data/usareur-tlaw-nj/*bro-known_services*"
    tags => [ "tlaw" ]
    tags => [ "bro_known_services" ]
  }
}

#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO_CONN FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-conn_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","proto","service","duration","orig_bytes","resp_bytes","conn_state","local_orig","local_resp","missed_bytes","history","orig_pkts","orig_ip_bytes","resp_pkts","resp_ip_bytes","tunnel_parents"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }

    #The following makes use of the translate filter (logstash contrib) to convert conn_state into human text. Saves having to look up values for packet introspection
    translate {
      field => "conn_state"

      destination => "conn_state_full"

      dictionary => [
                    "S0", "Connection attempt seen, no reply",
                    "S1", "Connection established, not terminated",
                    "S2", "Connection established and close attempt by originator seen (but no reply from responder)",
                    "S3", "Connection established and close attempt by responder seen (but no reply from originator)",
                    "SF", "Normal SYN/FIN completion",
                    "REJ", "Connection attempt rejected",
                    "RSTO", "Connection established, originator aborted (sent a RST)",
                    "RSTR", "Established, responder aborted",
                    "RSTOS0", "Originator sent a SYN followed by a RST, we never saw a SYN-ACK from the responder",
                    "RSTRH", "Responder sent a SYN ACK followed by a RST, we never saw a SYN from the (purported) originator",
                    "SH", "Originator sent a SYN followed by a FIN, we never saw a SYN ACK from the responder (hence the connection was 'half' open)",
                        "SHR", "Responder sent a SYN ACK followed by a FIN, we never saw a SYN from the originator",
                    "OTH", "No SYN seen, just midstream traffic (a 'partial connection' that was not later closed)"
                    ]
    }
  }
}

#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-HTTP FILTER EDITED 6/2/2016 JUARBE
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {
  #Takes in all type http-bro
  if [type] == "bro-http_log" {
    csv {

      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","trans_depth","method","host","uri","referrer","user_agent","request_body_len","response_body_len","status_code","status_message","info_code","info_msg","file_name","attributes","username","password","proxied","orig_fuids","orig_mime_types","resp_fuids","resp_mime_types"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }
   # This next section will match all legit parsed http bro logs that are tagged "bro_http" and will proceed to the output function
   if "bro_http" in [tags] {
    date {
      match => [ "ts", "UNIX" ]
    }

    #The following makes use of the translate filter (logstash contrib) to convert conn_state into human text. Saves having to look up values for packet introspection
    translate {
      field => "status_code"

	destination => "status_code_full"

    dictionary => [
      #1xx Codes Are Informational
      100, "Continue",
      101, "Switching Protocols",
      102, "Processing",
      
      #2xx Codes Are Successful; We Like These
      200, "OK",
      201, "Created",
      202, "Accepted",
      203, "Non-Authoritative Information",
      204, "No Content",
      205, "Reset Content",
      206, "Partial Content",
      207, "Multi-Status",
      208, "Already Reported",
      226, "Instance Manipulation Used",
      
      #3xx Codes Indicate that Further User Agent Action May be Needed
      300, "Multiple Choices",
      301, "Moved Permanently",
      302, "Not Found",
      303, "See Other",
      304, "Not Modified",
      305, "Use Proxy",
      #306 is no longer used, however is still reserved
      307, "Temporary Redirect",
      308, "Permanent Redirect",
      
      #4xx Codes Refer to Client Errors
      400, "Bad Request",
      401, "Unauthorized",
      402, "Payment Required",
      403, "Forbidden",
      404, "Not Found",
      405, "Method Not Allowed",
      406, "Not Acceptable",
      407, "Proxy Authentication Required",
      408, "Request Timeout",
      409, "Conflict",
      410, "Gone",
      411, "Length Required",
      412, "Precondition Failed",
      413, "Request Entity Too Large",
      414, "Request URI Too Long",
      415, "Unsupported Media Type",
      416, "Requested Range Not Satisfiable",
      417, "Expectation Failed",
      422, "Unprocessable Entity",
      423, "Locked",
      424, "Failed Dependency",
      426, "Upgrade Required",
      428, "Precondition Required",
      429, "Too Many Requests",
      431, "Request Header Fields Too Large",
      440, "Login Timeout",
      444, "No Response (This is specific to Nginx)",
      450, "Blocked my Microsoft Windows Parental Controls",
      
      #5xx Codes Refer to Server Errors
      500, "Internal Server Error",
      501, "Not Implemented",
      502, "Bad Gateway",
      503, "Service Unavailable",
      504, "Gateway Timeout",
      505, "HTTP Version Not Supported",
      506, "Variant Also Negotiates",
      507, "Insufficient Storage",
      508, "Loop Detected",
      510, "Not Extended",
      511, "Network Authentication Required"
		]
	}
	}
	##This is where if the weird looking HTTP logs come in, it will catch based off the CSVPARSEFAILURE
	if "_csvparsefailure" in [tags] {
		###THis will simpy grok match on timestamp and put the rest of the log on GREEDYDATA in http_message variable for later parsing
		grok {
			match => { "message" => "%{BASE10NUM:ts}%{GREEDYDATA:http_message}" }
		}
		####THis is where I use the gsub function. This will find using REGEX the \t generated from the csv parsing and replace then with tabs (whitespace)
		mutate {
			gsub => [
			"http_message", "[\\][a-z]", "	"			
			]
		}
		####this is where I then grok the tab deliminated fields to a known good point => up to the Method part of the http bro log field and the rest becomes greedy data again
		grok {
			match => { "http_message" => "%{DATA:ts}	%{DATA:uid}	%{IP:src_ip}	%{DATA:src_port}	%{IP:dst_ip}	%{DATA:dst_port}	%{DATA:trans_depth}	%{DATA:method}	%{GREEDYDATA:broken_http}" }
		}
		date {
			match => [ "ts", "UNIX" ]
		}
		##this is where i remove the tag labled _csvparsefailure and add the "weird_http" tag to better query in Kibana
		mutate {
			remove_tag => [ "_csvparsefailure" ]
			add_tag => [ "weird_http" ]
		}
	}
}
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-DNS FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-dns_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","proto","trans_id","query","qclass","qclass_name","qtype","qtype_name","rcode","rcode_name","AA","TC","RD","RA","Z","answers","TTLs","rejected"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
    }
  }
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-FILES FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-files_log" {
    csv {
      columns => ["ts","fuid","tx_hosts","rx_hosts","conn_uids","source","depth","analyzers","mime_type","file_names","duration","local_orig","is_orig","seen_bytes","total_bytes","missing_bytes","oveflow_bytes","timed_out","parent_fuid","md5","sha1","sha256","extracted"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "tx_hosts"
      target => "orig_geoip"
    }
    geoip {
      source => "rx_hosts"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-FTP FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-ftp_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","user","password","command","arg","mime_type","file_size","reply_code","reply_msg","data_channel_passive","data_channel_src_ip","data_channel_resp_ip","data_channel_resp_port","fuid"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-NOTICE FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-notice_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","fuid","file_mime_type","file_desc","proto","note","msg","sub","src","dst","p","n","peer_descr","actions","supress_for","dropped","remote_location_country_code","remote_location_region","remote_location_city","remote_location_latitude","remote_location_longitude"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-WEIRD FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-weird_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","name","addl","notice","peer"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-SNMP FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-snmp_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","duration","version","version","get_requests","get_bulk_requests","get_responses","set_requests","display_string","up_since"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-RADUIS FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-radius_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","username","mac","remote_ip","connect_info","result"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-SOCKS FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-socks_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","version","user","password","status","request_host","request_name","request_p","bound_host","bound_p"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-SMTP FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-smtp_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","trans_depth","helo","mailfrom","rcptto","date","from","to","reply_to","msg_id","in_reply_to","subject","x_originating_ip","first_received","second_received","last_reply","path","user_agent","tls","fuids","is_webmail"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }
#	date {
#		match => [ "date", "EEE, dd MMM yyyy HH:mm:ss ZZ" ]
#		target => "date"
#		}

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
#	mutate {
#		remove_tag => [ "_dateparsefailure" ]
#	}
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-SOFTWARE FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-software_log" {
    csv {
      columns => ["ts","host","host_port","software_type","name","version_major","version_minor","version_minor2","versino_minor3","version_addl","unparsed_version"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-SSH FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-ssh_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","version","auth_success","direction","client","server","cipher_alg","mac_alg","compression_alg","kex_log","host_key_alg","host_key","remote_location_country_code","remote_location_region","remote_location_city","remote_location_latitude","remote_location_longitude"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-REPORTER FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-reporter_log" {
    csv {
      columns => ["ts","level","message","location"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-SSL FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-ssl_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","version","cypher","curve","server_name","resumed","last_alert","next_protocol","established","cert_chain_fuids","client_cert_chain_fuids","subject","issuer","client_subject","client_issuer","validation_status"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-x509 FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-x509_log" {
    csv {
      columns => ["ts","id","certificate_version","certificate_serial","certificate_subject","certificate_issuer","certificate_not_valid_before","certificate_not_valid_after","certificate_key_alg","certificate_sig_alg","certificate_key_type","certificate_key_length","certificate_exponent","certificate_curve","san_dns","san_uri","san_email","san_ip","basic_constraints_ca","basic_constraints_path_len"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
		}
	}
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-TUNNEL FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-tunnel_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","tunnel_type","action"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-COMUNNICATION FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-communication_log" {
    csv {
      columns => ["ts","peer","src_name","connected_peer_desc","connected_peer_addr","connected_peer_port","level","message"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-DPD FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-dpd_log" {
    csv {
      columns => ["ts","uid","src_ip","src_port","dst_ip","dst_port","proto","analyzer","failure_reason"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }

    # add geoip attributes
    geoip {
      source => "src_ip"
      target => "orig_geoip"
    }
    geoip {
      source => "dst_ip"
      target => "resp_geoip"
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-KNOWN_HOSTS FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-known_hosts_log" {
    csv {
      columns => ["ts","host"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	BRO-KNOWN_SERVICES FILTER
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
filter {

  #Let's get rid of those header lines; they begin with a hash
  if [message] =~ /^#/ {
    drop { }
  }

  #Now, using the csv filter, we can define the Bro log fields
  if [type] == "bro-known_services_log" {
    csv {
      columns => ["ts","host","port_num","port_proto","service"]

      #If you use a custom delimiter, change the following value in between the quotes to your delimiter. Otherwise, insert a literal <tab> in between the two quotes on your logstash system, use a text editor like nano that doesn't convert tabs to spaces.
      separator => "	"
    }

    #Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
    date {
      match => [ "ts", "UNIX" ]
    }
  }
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	OUTPUT
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
output {
	#stdout { codec => rubydebug }
	elasticsearch { 
		hosts => "localhost:9200"
		index => "logstash-bro-%{+YYYY.MM.dd}"
	}
}
